{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RZIXFGgfZxL"
      },
      "source": [
        "###Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tZbqwCjfUvy",
        "outputId": "a5a499a8-68e7-4c60-ee98-c0fd46e70f4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFCGeGSkfVrA",
        "outputId": "7a87405c-f647-41a4-eada-7d9af2632acd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/vqa\n"
          ]
        }
      ],
      "source": [
        "# Since we've shared this drive with you, please use the correct file path from your drive since it'll go to SharedDrive for you\n",
        "%cd '/content/drive/MyDrive/vqa'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting opencv-python\n",
            "  Obtaining dependency information for opencv-python from https://files.pythonhosted.org/packages/a1/f6/57de91ea40c670527cd47a6548bf2cbedc68cec57c041793b256356abad7/opencv_python-4.8.1.78-cp37-abi3-macosx_11_0_arm64.whl.metadata\n",
            "  Downloading opencv_python-4.8.1.78-cp37-abi3-macosx_11_0_arm64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /Users/sh69/miniconda3/envs/vqa/lib/python3.10/site-packages (from opencv-python) (1.24.3)\n",
            "Downloading opencv_python-4.8.1.78-cp37-abi3-macosx_11_0_arm64.whl (33.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hInstalling collected packages: opencv-python\n",
            "Successfully installed opencv-python-4.8.1.78\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RXIcQdTzwzor"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#os.chdir(\"/content/drive/MyDrive/vqa\")\n",
        "os.chdir(\"./\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeENDeNpLPbb"
      },
      "source": [
        "###Import relevant libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EWPWCQJ9LM7W"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from PIL import Image\n",
        "import re\n",
        "import time\n",
        "import cv2\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBtSbtFJMQ9U"
      },
      "source": [
        "###Setting the constants and parameter values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4kyUxL5EMVqW"
      },
      "outputs": [],
      "source": [
        "input_dir = './input_dir/'\n",
        "log_dir = './logs'\n",
        "model_dir = './models'\n",
        "# maximum length of question, the length in the VQA dataset is 26\n",
        "max_qst_length = 30\n",
        "# maximum number of answers\n",
        "max_num_ans = 10\n",
        "# embedding size of feature vector for image and question\n",
        "embed_size = 1024\n",
        "# embedding size of the word used as the input for the LSTM\n",
        "word_embed_size = 300\n",
        "# Number of layers in the LSTM\n",
        "num_layers = 2\n",
        "# Hidden size in the LSTM\n",
        "hidden_size = 64\n",
        "# Learning rate, step size and decay rate used while initializing the Step learning rate Scheduler\n",
        "learning_rate = 0.001\n",
        "step_size = 10\n",
        "gamma = 0.1\n",
        "#Number of epochs it is trained on\n",
        "num_epochs = 30\n",
        "#Batch size, number of workers and the steps after which the model parameters are saved\n",
        "batch_size = 256\n",
        "num_workers = 4\n",
        "save_step = 1\n",
        "\n",
        "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device = torch.device('mps')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byQqyFa2LyJ4"
      },
      "source": [
        "###Helper Functions for Handling Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nO1Aqs0QLsWM"
      },
      "outputs": [],
      "source": [
        "SENTENCE_SPLIT_REGEX = re.compile(r'(\\W+)')\n",
        "\n",
        "# create tokens\n",
        "def tokenize(sentence):\n",
        "    tokens = SENTENCE_SPLIT_REGEX.split(sentence.lower())\n",
        "    tokens = [t.strip() for t in tokens if len(t.strip()) > 0]\n",
        "    return tokens\n",
        "\n",
        "# returns a file as list of lines\n",
        "def load_str_list(fname):\n",
        "    with open(fname) as f:\n",
        "        lines = f.readlines()\n",
        "    lines = [l.strip() for l in lines]\n",
        "    return lines\n",
        "\n",
        "\n",
        "# Tokenizes the text and then gives the index of the word from the vocab txt file of answers and questions\n",
        "class VocabDict:\n",
        "\n",
        "    def __init__(self, vocab_file):\n",
        "        self.word_list = load_str_list(vocab_file)\n",
        "        self.word2idx_dict = {w:n_w for n_w, w in enumerate(self.word_list)}\n",
        "        self.vocab_size = len(self.word_list)\n",
        "        self.unk2idx = self.word2idx_dict['<unk>'] if '<unk>' in self.word2idx_dict else None\n",
        "\n",
        "    def idx2word(self, n_w):\n",
        "\n",
        "        return self.word_list[n_w]\n",
        "\n",
        "    def word2idx(self, w):\n",
        "        if w in self.word2idx_dict:\n",
        "            return self.word2idx_dict[w]\n",
        "        elif self.unk2idx is not None:\n",
        "            return self.unk2idx\n",
        "        else:\n",
        "            raise ValueError('word %s not in dictionary (while dictionary does not contain <unk>)' % w)\n",
        "\n",
        "    def tokenize_and_index(self, sentence):\n",
        "        inds = [self.word2idx(w) for w in tokenize(sentence)]\n",
        "\n",
        "        return inds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMKzQIZZMCyh"
      },
      "source": [
        "###Building the Dataset and DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "j2ri8KxPL3xH"
      },
      "outputs": [],
      "source": [
        "from data_loader import VqaDataset,get_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kubGfT_R0yQ"
      },
      "source": [
        "###Model: Image Encoding Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "3crpWchKR1Ua"
      },
      "outputs": [],
      "source": [
        "#Block 1: Image Channel which creates the image embedding\n",
        "class ImgEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size):\n",
        "        super(ImgEncoder, self).__init__()\n",
        "        model = models.vgg19(pretrained=True)\n",
        "        in_features = model.classifier[-1].in_features  # input size of feature vector\n",
        "        model.classifier = nn.Sequential(\n",
        "            *list(model.classifier.children())[:-1])    # remove last fc layer\n",
        "\n",
        "        self.model = model                              # loaded model without last fc layer\n",
        "        self.fc = nn.Linear(in_features, embed_size)    # feature vector of image\n",
        "\n",
        "    def forward(self, image):\n",
        "\n",
        "        with torch.no_grad():\n",
        "            img_feature = self.model(image)                  # [batch_size, vgg16(19)_fc=4096]\n",
        "        img_feature = self.fc(img_feature)                   # [batch_size, embed_size]\n",
        "\n",
        "        l2_norm = img_feature.norm(p=2, dim=1, keepdim=True).detach()\n",
        "        img_feature = img_feature.div(l2_norm)               # l2-normalized feature vector\n",
        "\n",
        "        return img_feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS-9L7-_SmpP"
      },
      "source": [
        "###Model: Question Encoding Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "InpvRJdcSqPD"
      },
      "outputs": [],
      "source": [
        "class QstEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, qst_vocab_size, word_embed_size, embed_size, num_layers, hidden_size):\n",
        "\n",
        "        super(QstEncoder, self).__init__()\n",
        "        self.word2vec = nn.Embedding(qst_vocab_size, word_embed_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.lstm = nn.LSTM(word_embed_size, hidden_size, num_layers)\n",
        "        self.fc = nn.Linear(2*num_layers*hidden_size, embed_size)     # 2 for hidden and cell states\n",
        "\n",
        "    def forward(self, question):\n",
        "\n",
        "        qst_vec = self.word2vec(question)                             # [batch_size, max_qst_length=30, word_embed_size=300]\n",
        "        qst_vec = self.tanh(qst_vec)\n",
        "        qst_vec = qst_vec.transpose(0, 1)                             # [max_qst_length=30, batch_size, word_embed_size=300]\n",
        "        _, (hidden, cell) = self.lstm(qst_vec)                        # [num_layers=2, batch_size, hidden_size=512]\n",
        "        qst_feature = torch.cat((hidden, cell), 2)                    # [num_layers=2, batch_size, 2*hidden_size=1024]\n",
        "        qst_feature = qst_feature.transpose(0, 1)                     # [batch_size, num_layers=2, 2*hidden_size=1024]\n",
        "        qst_feature = qst_feature.reshape(qst_feature.size()[0], -1)  # [batch_size, 2*num_layers*hidden_size=2048]\n",
        "        qst_feature = self.tanh(qst_feature)\n",
        "        qst_feature = self.fc(qst_feature)                            # [batch_size, embed_size]\n",
        "\n",
        "        return qst_feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIFmwwSJS_9s"
      },
      "source": [
        "###Model: Combine Image Encoding and Question Encoding Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "C57q-LSBTE9g"
      },
      "outputs": [],
      "source": [
        "class VqaModel(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size, qst_vocab_size, ans_vocab_size, word_embed_size, num_layers, hidden_size):\n",
        "\n",
        "        super(VqaModel, self).__init__()\n",
        "        self.img_encoder = ImgEncoder(embed_size)\n",
        "        self.qst_encoder = QstEncoder(qst_vocab_size, word_embed_size, embed_size, num_layers, hidden_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(embed_size, ans_vocab_size)\n",
        "        self.fc2 = nn.Linear(ans_vocab_size, ans_vocab_size)\n",
        "\n",
        "    def forward(self, img, qst):\n",
        "\n",
        "        img_feature = self.img_encoder(img)                     # [batch_size, embed_size]\n",
        "        qst_feature = self.qst_encoder(qst)                     # [batch_size, embed_size]\n",
        "\n",
        "        # Elementwise multiplication of image and question vectors for fusion\n",
        "        combined_feature = torch.mul(img_feature, qst_feature)  # [batch_size, embed_size]\n",
        "        combined_feature = self.tanh(combined_feature)\n",
        "        combined_feature = self.dropout(combined_feature)\n",
        "        combined_feature = self.fc1(combined_feature)           # [batch_size, ans_vocab_size=1000]\n",
        "        combined_feature = self.tanh(combined_feature)\n",
        "        combined_feature = self.dropout(combined_feature)\n",
        "        combined_feature = self.fc2(combined_feature)           # [batch_size, ans_vocab_size=1000]\n",
        "\n",
        "        return combined_feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r97BtjwqxSew",
        "outputId": "b8d96941-3e94-45ee-aec4-1c52ac5ee780"
      },
      "outputs": [],
      "source": [
        "# Get data loader for train and test - it's a dictionary with the key train having the train dataloader and same for test\n",
        "data_loader = get_loader(\n",
        "        input_dir=input_dir,\n",
        "        input_vqa_train='train.npy',\n",
        "        input_vqa_valid='valid.npy',\n",
        "        max_qst_length=max_qst_length,\n",
        "        max_num_ans=max_num_ans,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers)\n",
        "\n",
        "qst_vocab_size = data_loader['train'].dataset.qst_vocab.vocab_size\n",
        "ans_vocab_size = data_loader['train'].dataset.ans_vocab.vocab_size\n",
        "ans_unk_idx = data_loader['train'].dataset.ans_vocab.unk2idx\n",
        "\n",
        "# Initializing the model\n",
        "model = VqaModel(\n",
        "        embed_size=embed_size,\n",
        "        qst_vocab_size=qst_vocab_size,\n",
        "        ans_vocab_size=ans_vocab_size,\n",
        "        word_embed_size=word_embed_size,\n",
        "        num_layers=num_layers,\n",
        "        hidden_size=hidden_size).to(device)\n",
        "\n",
        "# Initializing the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Choosing which parameters to update in the optimizer\n",
        "params = list(model.img_encoder.fc.parameters()) \\\n",
        "      + list(model.qst_encoder.parameters()) \\\n",
        "      + list(model.fc1.parameters()) \\\n",
        "      + list(model.fc2.parameters())\n",
        "\n",
        "# Initializing the optimizer and learning rate scheduler\n",
        "optimizer = optim.Adam(params, lr=learning_rate)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "last_time = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2JJKjmqPBTM"
      },
      "source": [
        "###Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "id": "auJoIXFiPAhE",
        "outputId": "0ce818f1-da25-4f98-ffac-79876210c503"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| TRAIN SET | Epoch [01/30], Step [0000/0057], Loss: 6.8989, Time left: 81396587.76 hr\n",
            "| TRAIN SET | Epoch [01/30], Step [0010/0057], Loss: 4.3280, Time left: 1.10 hr\n",
            "| TRAIN SET | Epoch [01/30], Step [0020/0057], Loss: 4.5443, Time left: 0.83 hr\n",
            "| TRAIN SET | Epoch [01/30], Step [0030/0057], Loss: 4.2608, Time left: 0.63 hr\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    for phase in ['train', 'valid']:\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corr = 0\n",
        "\n",
        "        batch_step_size = len(data_loader[phase].dataset) / batch_size\n",
        "\n",
        "        if phase == 'train':\n",
        "            scheduler.step()\n",
        "            model.train()\n",
        "        else:\n",
        "            model.eval()\n",
        "\n",
        "        for batch_idx, batch_sample in enumerate(data_loader[phase]):\n",
        "\n",
        "            image = batch_sample['image'].to(device)\n",
        "            question = batch_sample['question'].to(device)\n",
        "            label = batch_sample['answer_label'].to(device)\n",
        "            multi_choice = batch_sample['answer_multi_choice']  # not tensor, list.\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "                output = model(image, question)      # size: [batch_size X ans_vocab_size=1000]\n",
        "                _, pred = torch.max(output, 1)  # size: [batch_size]\n",
        "\n",
        "                loss = criterion(output, label)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            # Evaluation metric\n",
        "            running_loss += loss.item()\n",
        "            running_corr += torch.stack([(ans == pred.cpu()) for ans in multi_choice]).any(dim=0).sum()\n",
        "\n",
        "            # Print the average loss in a mini-batch.\n",
        "            if batch_idx % 10 == 0:\n",
        "                time_taken = time.time() - last_time\n",
        "                time_left = (((batch_step_size - batch_idx) * time_taken)/10) * (num_epochs - epoch)\n",
        "                print('| {} SET | Epoch [{:02d}/{:02d}], Step [{:04d}/{:04d}], Loss: {:.4f}, Time left: {:.2f} hr'\n",
        "                      .format(phase.upper(), epoch+1, num_epochs, batch_idx, int(batch_step_size), loss.item(), time_left/3600))\n",
        "                last_time = time.time()\n",
        "        # Print the average loss and accuracy in an epoch.\n",
        "        epoch_loss = running_loss / batch_step_size\n",
        "        epoch_acc = running_corr.double() / len(data_loader[phase].dataset)\n",
        "\n",
        "        print('| {} SET | Epoch [{:02d}/{:02d}], Loss: {:.4f}, Acc: {:.4f}\\n'\n",
        "              .format(phase.upper(), epoch+1, num_epochs, epoch_loss, epoch_acc))\n",
        "\n",
        "\n",
        "\n",
        "        # Log the loss and accuracy in an epoch.\n",
        "        with open(os.path.join(log_dir, '{}-log-epoch-{:02}.txt')\n",
        "                  .format(phase, epoch+1), 'w') as f:\n",
        "            f.write(str(epoch+1) + '\\t'\n",
        "                    + str(epoch_loss) + '\\t'\n",
        "                    + str(epoch_acc.item()))\n",
        "\n",
        "\n",
        "    # Save the model check points.\n",
        "    if (epoch+1) % save_step == 0:\n",
        "        torch.save(model, os.path.join(model_dir, '-epoch-{:02d}.pt'.format(epoch+1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya37IFIqTca8"
      },
      "source": [
        "###Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-v5XEy_EUs2U"
      },
      "outputs": [],
      "source": [
        "image_path = './input_dir/Resized_Images/test_img.jpeg'\n",
        "question = 'What does the sign say?'\n",
        "saved_model = './models/best_modelb.pt'\n",
        "max_qst_length=30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQvOBxL4T1BW",
        "outputId": "2118f5a8-fc1e-4a70-c5a6-1bdd20b5c53f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "predicted - probabilty\n",
            "'yes' - 0.3498\n",
            "'no' - 0.2768\n",
            "'green' - 0.0493\n",
            "'4' - 0.0321\n",
            "'1' - 0.0250\n"
          ]
        }
      ],
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "qst_vocab = load_str_list(\"/content/drive/MyDrive/vqa/dataset/vocab_questions.txt\")\n",
        "ans_vocab = load_str_list(\"/content/drive/MyDrive/vqa/dataset/vocab_answers.txt\")\n",
        "word2idx_dict = {w:n_w for n_w, w in enumerate(qst_vocab)}\n",
        "unk2idx = word2idx_dict['<unk>'] if '<unk>' in word2idx_dict else None\n",
        "qst_vocab_size = len(qst_vocab)\n",
        "ans_vocab_size = len(ans_vocab)\n",
        "\n",
        "def word2idx(w):\n",
        "        if w in word2idx_dict:\n",
        "            return word2idx_dict[w]\n",
        "        elif unk2idx is not None:\n",
        "            return unk2idx\n",
        "        else:\n",
        "            raise ValueError('word %s not in dictionary (while dictionary does not contain <unk>)' % w)\n",
        "\n",
        "image = cv2.imread(image_path)\n",
        "image = cv2.resize(image, dsize=(224,224), interpolation = cv2.INTER_AREA)\n",
        "image = torch.from_numpy(image).float()\n",
        "image = image.to(device)\n",
        "image = image.unsqueeze(dim=0)\n",
        "image = image.view(1,3,224,224)\n",
        "\n",
        "try:\n",
        "  q_list = list(question.split(\" \"))\n",
        "except:\n",
        "  q_list = list(question.split(1))\n",
        "\n",
        "idx = 'valid'\n",
        "qst2idc = np.array([word2idx('<pad>')] * max_qst_length)  # padded with '<pad>' in 'ans_vocab'\n",
        "qst2idc[:len(q_list)] = [word2idx(w) for w in q_list]\n",
        "\n",
        "question = qst2idc\n",
        "question = torch.from_numpy(question).long()\n",
        "\n",
        "question = question.to(device)\n",
        "question = question.unsqueeze(dim=0)\n",
        "\n",
        "net = torch.load(saved_model)\n",
        "net = net.to(device)\n",
        "\n",
        "net.eval()\n",
        "\n",
        "output = net(image, question)\n",
        "predicts = torch.softmax(output, 1)\n",
        "probs, indices = torch.topk(predicts, k=5, dim=1)\n",
        "probs = probs.squeeze()\n",
        "indices = indices.squeeze()\n",
        "print(\"predicted - probabilty\")\n",
        "for i in range(5):\n",
        "    print(\"'{}' - {:.4f}\".format(ans_vocab[indices[i].item()], probs[i].item()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.13 ('vqa')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "c5296ecca0b00e8539e0973b2cc4a7fca8404f4c460630821a17ec6b54a1d42b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
