{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from PIL import Image\n",
    "import re\n",
    "import time\n",
    "import cv2\n",
    "import warnings\n",
    "\n",
    "from data_loader import get_loader  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_dir = \"/content/drive/MyDrive/vqa\"\n",
    "#log_dir = \"/content/drive/MyDrive/vqa/logs\"\n",
    "#model_dir = \"/content/drive/MyDrive/vqa/models\"\n",
    "\n",
    "# maximum length of question, the length in the VQA dataset is 26\n",
    "max_qst_length = 30\n",
    "# maximum number of answers\n",
    "max_num_ans = 10\n",
    "# embedding size of feature vector for image and question\n",
    "embed_size = 1024\n",
    "# embedding size of the word used as the input for the LSTM\n",
    "word_embed_size = 300\n",
    "# Number of layers in the LSTM\n",
    "num_layers = 2\n",
    "# Hidden size in the LSTM\n",
    "hidden_size = 64\n",
    "# Learning rate, step size and decay rate used while initializing the Step learning rate Scheduler\n",
    "learning_rate = 0.001\n",
    "step_size = 10\n",
    "gamma = 0.1\n",
    "# Number of epochs it is trained on\n",
    "num_epochs = 30\n",
    "# Batch size, number of workers and the steps after which the model parameters are saved\n",
    "batch_size = 256\n",
    "num_workers = 4\n",
    "save_step = 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgEncoder(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(ImgEncoder, self).__init__()\n",
    "        model = models.vgg19(pretrained=True)\n",
    "        in_features = model.classifier[-1].in_features  # input size of feature vector\n",
    "        model.classifier = nn.Sequential(\n",
    "            *list(model.classifier.children())[:-1]\n",
    "        )  # remove last fc layer\n",
    "\n",
    "        self.model = model  # loaded model without last fc layer\n",
    "        self.fc = nn.Linear(in_features, embed_size)  # feature vector of image\n",
    "\n",
    "    def forward(self, image):\n",
    "        with torch.no_grad():\n",
    "            img_feature = self.model(image)  # [batch_size, vgg16(19)_fc=4096]\n",
    "        img_feature = self.fc(img_feature)  # [batch_size, embed_size]\n",
    "\n",
    "        l2_norm = img_feature.norm(p=2, dim=1, keepdim=True).detach()\n",
    "        img_feature = img_feature.div(l2_norm)  # l2-normalized feature vector\n",
    "\n",
    "        return img_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question Encoding Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QstEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, qst_vocab_size, word_embed_size, embed_size, num_layers, hidden_size\n",
    "    ):\n",
    "        super(QstEncoder, self).__init__()\n",
    "        self.word2vec = nn.Embedding(qst_vocab_size, word_embed_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.lstm = nn.LSTM(word_embed_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(\n",
    "            2 * num_layers * hidden_size, embed_size\n",
    "        )  # 2 for hidden and cell states\n",
    "\n",
    "    def forward(self, question):\n",
    "        qst_vec = self.word2vec(\n",
    "            question\n",
    "        )  # [batch_size, max_qst_length=30, word_embed_size=300]\n",
    "        qst_vec = self.tanh(qst_vec)\n",
    "        qst_vec = qst_vec.transpose(\n",
    "            0, 1\n",
    "        )  # [max_qst_length=30, batch_size, word_embed_size=300]\n",
    "        _, (hidden, cell) = self.lstm(\n",
    "            qst_vec\n",
    "        )  # [num_layers=2, batch_size, hidden_size=512]\n",
    "        qst_feature = torch.cat(\n",
    "            (hidden, cell), 2\n",
    "        )  # [num_layers=2, batch_size, 2*hidden_size=1024]\n",
    "        qst_feature = qst_feature.transpose(\n",
    "            0, 1\n",
    "        )  # [batch_size, num_layers=2, 2*hidden_size=1024]\n",
    "        qst_feature = qst_feature.reshape(\n",
    "            qst_feature.size()[0], -1\n",
    "        )  # [batch_size, 2*num_layers*hidden_size=2048]\n",
    "        qst_feature = self.tanh(qst_feature)\n",
    "        qst_feature = self.fc(qst_feature)  # [batch_size, embed_size]\n",
    "\n",
    "        return qst_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vqa model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VqaModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_size,\n",
    "        qst_vocab_size,\n",
    "        ans_vocab_size,\n",
    "        word_embed_size,\n",
    "        num_layers,\n",
    "        hidden_size,\n",
    "    ):\n",
    "        super(VqaModel, self).__init__()\n",
    "        self.img_encoder = ImgEncoder(embed_size)\n",
    "        self.qst_encoder = QstEncoder(\n",
    "            qst_vocab_size, word_embed_size, embed_size, num_layers, hidden_size\n",
    "        )\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(embed_size, ans_vocab_size)\n",
    "        self.fc2 = nn.Linear(ans_vocab_size, ans_vocab_size)\n",
    "\n",
    "    def forward(self, img, qst):\n",
    "        img_feature = self.img_encoder(img)  # [batch_size, embed_size]\n",
    "        qst_feature = self.qst_encoder(qst)  # [batch_size, embed_size]\n",
    "\n",
    "        # Elementwise multiplication of image and question vectors for fusion\n",
    "        combined_feature = torch.mul(\n",
    "            img_feature, qst_feature\n",
    "        )  # [batch_size, embed_size]\n",
    "        combined_feature = self.tanh(combined_feature)\n",
    "        combined_feature = self.dropout(combined_feature)\n",
    "        combined_feature = self.fc1(\n",
    "            combined_feature\n",
    "        )  # [batch_size, ans_vocab_size=1000]\n",
    "        combined_feature = self.tanh(combined_feature)\n",
    "        combined_feature = self.dropout(combined_feature)\n",
    "        combined_feature = self.fc2(\n",
    "            combined_feature\n",
    "        )  # [batch_size, ans_vocab_size=1000]\n",
    "\n",
    "        return combined_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data loader for train and test - it's a dictionary with the key train having the train dataloader and same for test\n",
    "data_loader = get_loader(\n",
    "    input_dir=input_dir,\n",
    "    input_vqa_train=\"dataset/train.npy\",\n",
    "    input_vqa_valid=\"dataset/valid.npy\",\n",
    "    max_qst_length=max_qst_length,\n",
    "    max_num_ans=max_num_ans,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "qst_vocab_size = data_loader[\"train\"].dataset.qst_vocab.vocab_size\n",
    "ans_vocab_size = data_loader[\"train\"].dataset.ans_vocab.vocab_size\n",
    "ans_unk_idx = data_loader[\"train\"].dataset.ans_vocab.unk2idx\n",
    "\n",
    "# Initializing the model\n",
    "model = VqaModel(\n",
    "    embed_size=embed_size,\n",
    "    qst_vocab_size=qst_vocab_size,\n",
    "    ans_vocab_size=ans_vocab_size,\n",
    "    word_embed_size=word_embed_size,\n",
    "    num_layers=num_layers,\n",
    "    hidden_size=hidden_size,\n",
    ").to(device)\n",
    "\n",
    "# Initializing the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Choosing which parameters to update in the optimizer\n",
    "params = (\n",
    "    list(model.img_encoder.fc.parameters())\n",
    "    + list(model.qst_encoder.parameters())\n",
    "    + list(model.fc1.parameters())\n",
    "    + list(model.fc2.parameters())\n",
    ")\n",
    "\n",
    "# Initializing the optimizer and learning rate scheduler\n",
    "optimizer = optim.Adam(params, lr=learning_rate)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "last_time = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for phase in [\"train\", \"valid\"]:\n",
    "        running_loss = 0.0\n",
    "        running_corr = 0\n",
    "\n",
    "        batch_step_size = len(data_loader[phase].dataset) / batch_size\n",
    "\n",
    "        if phase == \"train\":\n",
    "            scheduler.step()\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        for batch_idx, batch_sample in enumerate(data_loader[phase]):\n",
    "            image = batch_sample[\"image\"].to(device)\n",
    "            question = batch_sample[\"question\"].to(device)\n",
    "            label = batch_sample[\"answer_label\"].to(device)\n",
    "            multi_choice = batch_sample[\"answer_multi_choice\"]  # not tensor, list.\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == \"train\"):\n",
    "                output = model(\n",
    "                    image, question\n",
    "                )  # size: [batch_size X ans_vocab_size=1000]\n",
    "                _, pred = torch.max(output, 1)  # size: [batch_size]\n",
    "\n",
    "                loss = criterion(output, label)\n",
    "\n",
    "                if phase == \"train\":\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            # Evaluation metric\n",
    "            running_loss += loss.item()\n",
    "            running_corr += (\n",
    "                torch.stack([(ans == pred.cpu()) for ans in multi_choice])\n",
    "                .any(dim=0)\n",
    "                .sum()\n",
    "            )\n",
    "\n",
    "            # Print the average loss in a mini-batch.\n",
    "            if batch_idx % 10 == 0:\n",
    "                time_taken = time.time() - last_time\n",
    "                time_left = (((batch_step_size - batch_idx) * time_taken) / 10) * (\n",
    "                    num_epochs - epoch\n",
    "                )\n",
    "                print(\n",
    "                    \"| {} SET | Epoch [{:02d}/{:02d}], Step [{:04d}/{:04d}], Loss: {:.4f}, Time left: {:.2f} hr\".format(\n",
    "                        phase.upper(),\n",
    "                        epoch + 1,\n",
    "                        num_epochs,\n",
    "                        batch_idx,\n",
    "                        int(batch_step_size),\n",
    "                        loss.item(),\n",
    "                        time_left / 3600,\n",
    "                    )\n",
    "                )\n",
    "                last_time = time.time()\n",
    "        # Print the average loss and accuracy in an epoch.\n",
    "        epoch_loss = running_loss / batch_step_size\n",
    "        epoch_acc = running_corr.double() / len(data_loader[phase].dataset)\n",
    "\n",
    "        print(\n",
    "            \"| {} SET | Epoch [{:02d}/{:02d}], Loss: {:.4f}, Acc: {:.4f}\\n\".format(\n",
    "                phase.upper(), epoch + 1, num_epochs, epoch_loss, epoch_acc\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Log the loss and accuracy in an epoch.\n",
    "        with open(\n",
    "            os.path.join(log_dir, \"{}-log-epoch-{:02}.txt\").format(phase, epoch + 1),\n",
    "            \"w\",\n",
    "        ) as f:\n",
    "            f.write(\n",
    "                str(epoch + 1) + \"\\t\" + str(epoch_loss) + \"\\t\" + str(epoch_acc.item())\n",
    "            )\n",
    "\n",
    "    # Save the model check points.\n",
    "    if (epoch + 1) % save_step == 0:\n",
    "        torch.save(model, os.path.join(model_dir, \"-epoch-{:02d}.pt\".format(epoch + 1)))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
